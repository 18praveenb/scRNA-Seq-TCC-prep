{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip, time, random\n",
    "from multiprocessing import Pool\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read config.json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"config.json\") as json_file:\n",
    "    parameter = json.load(json_file)\n",
    "\n",
    "\n",
    "print \"READ FILES:\\n\"\n",
    "read_dirs=[]\n",
    "for i in range(len(parameter[\"read_filenames\"])):\n",
    "    read_dirs+=[str(parameter[\"BASE_DIR\"])+str(parameter[\"read_filenames\"][i])]\n",
    "    print read_dirs[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "random.seed()\n",
    "BARCODE_LENGTH=parameter['BARCODE_LENGTH']\n",
    "output_dir = parameter['OUTPUT_DIR']\n",
    "NUM_THREADS = parameter['NUM_THREADS']\n",
    "#temporary file to extract all reads\n",
    "all_reads_file = str(parameter[\"BASE_DIR\"])+'all_reads.fastq'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def encoding_map(ch):\n",
    "    if ch=='A':return 0\n",
    "    if ch=='G':return 1\n",
    "    if ch=='C':return 2\n",
    "    if ch=='T':return 3\n",
    "    if ch=='N':return random.randint(0,3)\n",
    "\n",
    "decoding_lst = ['A', 'G', 'C', 'T']\n",
    "\n",
    "def encode(k):\n",
    "    code = 0\n",
    "    for ch in k:\n",
    "        code *= 4\n",
    "        code += encoding_map(ch)\n",
    "    return code\n",
    "\n",
    "def decode(code):\n",
    "    ret = ''\n",
    "    for _ in range(14):\n",
    "        index = code & 3\n",
    "        code >>= 2\n",
    "        ret = decoding_lst[index] + ret\n",
    "    return ret\n",
    "\n",
    "\n",
    "#LOAD barcodes\n",
    "save_dir=str(parameter[\"SAVE_DIR\"])\n",
    "\n",
    "print \"Loading Barcodes...\"\n",
    "t0 = time.time()\n",
    "with open(save_dir+\"barcodes.dat\", 'rb') as f:\n",
    "    barcodes=pickle.load(f)\n",
    "with open(save_dir+\"codewords.dat\", 'rb') as f:\n",
    "    codewords=pickle.load(f)\n",
    "with open(save_dir+\"brc_idx_to_correct.dat\", 'rb') as f:\n",
    "    brc_idx_to_correct= pickle.load(f)\n",
    "t1 = time.time()\n",
    "print t1-t0, \"sec\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error-correct barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_barcodes(id):\n",
    "    if id in brc_idx_to_correct:\n",
    "        s=set(mutations(decode(codewords[id]),1))\n",
    "        pos=[]\n",
    "        for idx, barcode in enumerate(barcodes):\n",
    "            if barcode in s:\n",
    "                pos+=[idx]\n",
    "        return pos\n",
    "    else:\n",
    "        s=codewords[id]\n",
    "        return [idx for idx, barcode in enumerate(barcodes) if barcode == s]\n",
    "    \n",
    "    \n",
    "import itertools\n",
    "def mutations(word, hamming_distance, charset='ATCG'):\n",
    "    for indices in itertools.combinations(range(len(word)), hamming_distance):\n",
    "        for replacements in itertools.product(charset, repeat=hamming_distance):\n",
    "            mutation = list(word)\n",
    "            for index, replacement in zip(indices, replacements):\n",
    "                mutation[index] = replacement\n",
    "            yield encode(\"\".join(mutation))\n",
    "\n",
    "\n",
    "\n",
    "print \"Merging barcodes... NUM_THREADS =\",NUM_THREADS \n",
    "p = Pool(NUM_THREADS)\n",
    "t0 = time.time()\n",
    "ret_vec=p.map(merge_barcodes, range(len(codewords)))\n",
    "t1 = time.time()\n",
    "print t1-t0, \"sec\"\n",
    "p.close(); p.join()\n",
    "\n",
    "\n",
    "reads_per_barcode=[]\n",
    "for i in range(len(codewords)):\n",
    "    reads_per_barcode+=[len(ret_vec[i])]\n",
    "print \"Reads in Barcodes:\",sum(reads_per_barcode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output single-cell files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create output directory \n",
    "import os\n",
    "if not os.path.isdir(output_dir):\n",
    "    try:\n",
    "        os.mkdir(output_dir)\n",
    "    except OSError as e:\n",
    "        print \"OSError({0}): {1}\".format(e.errno, e.strerror)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#concatenate all .gz read files\n",
    "\n",
    "command = \"cat \"\n",
    "for files in [read_dirs[0],read_dirs[1],read_dirs[2],read_dirs[3],read_dirs[4],read_dirs[5],read_dirs[6],read_dirs[7]]:  \n",
    "    command+=files+' '\n",
    "command+=\"> \"+all_reads_file+\".gz\"\n",
    "print \"cat...\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temporarilly unzip all reads\n",
    "\n",
    "t0=time.time()\n",
    "print \"gunzip...\"\n",
    "\n",
    "os.system(\"gunzip \"+all_reads_file+\".gz\")\n",
    "\n",
    "t1=time.time()\n",
    "print t1-t0, \"sec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create line_offset list\n",
    "\n",
    "f = open(all_reads_file)\n",
    "\n",
    "t0=time.time()\n",
    "print \"line_offset...\"\n",
    "line_offset = []\n",
    "offset = 0\n",
    "for line in f:\n",
    "    line_offset.append(offset)\n",
    "    offset += len(line)\n",
    "    \n",
    "f.close()\n",
    "t1=time.time()\n",
    "print t1-t0, \"sec\"  \n",
    "\n",
    "\n",
    "NUM_OF_LINES=len(line_offset)\n",
    "print \"number of reads in dataset =\",NUM_OF_LINES/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split single-cell files and umis\n",
    "\n",
    "f = open(all_reads_file)\n",
    "t0=time.time()\n",
    "for cell in range(len(codewords)):\n",
    "    filename = \"cell_\"+str(cell).zfill(4)+'_'+decode(codewords[cell])\n",
    "    print \"writing \" + filename +\"...\"\n",
    "    output_umis=\"\"\n",
    "    output_fastq=\"\"\n",
    "    for i in ret_vec[cell]:\n",
    "        f.seek(line_offset[i*8])\n",
    "        output_fastq+=f.readline()\n",
    "        output_fastq+=f.readline()\n",
    "        output_fastq+=f.readline()\n",
    "        output_fastq+=f.readline()\n",
    "\n",
    "        f.seek(line_offset[5+i*8])\n",
    "        output_umis+=f.readline()\n",
    "    \n",
    "    with open(output_dir+filename+\".umi\", 'wb') as umi:\n",
    "        umi.write(output_umis)\n",
    "    with open(output_dir+filename+\".fastq\", 'wb') as reads:\n",
    "        reads.write(output_fastq)\n",
    "\n",
    "f.close()\n",
    "t1=time.time()\n",
    "print t1-t0, \"sec\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove temp all_reads file \n",
    "os.system(\"rm \"+all_reads_file)\n",
    "\n",
    "\n",
    "#compress output files \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "fastqfiles = [output_dir+f for f in listdir(output_dir) if isfile(join(output_dir, f)) and f[-6:]==\".fastq\"]\n",
    "\n",
    "def gzip_fastqs(filepath):\n",
    "    if filepath[-6:]==\".fastq\":\n",
    "        os.system(\"gzip \"+ filepath)\n",
    "\n",
    "print \"gzip...\"\n",
    "\n",
    "p=Pool(8)\n",
    "t0 = time.time()\n",
    "p.map(gzip_fastqs, fastqfiles)\n",
    "t1 = time.time()\n",
    "print t1-t0, \"sec\"\n",
    "p.close(); p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fastqfiles = [output_dir+f for f in sorted(listdir(output_dir)) if isfile(join(output_dir, f)) and f[-9:]==\".fastq.gz\"]\n",
    "umifiles = [output_dir+f for f in sorted(listdir(output_dir)) if isfile(join(output_dir, f)) and f[-4:]==\".umi\"]\n",
    "cell_ids = [f[:24] for f in sorted(listdir(output_dir)) if isfile(join(output_dir, f)) and f[-4:]==\".umi\"]\n",
    "\n",
    "out_data=''\n",
    "for i in range(len(cell_ids)):\n",
    "    out_data+=cell_ids[i]+'\\t'+umifiles[i]+'\\t'+fastqfiles[i]+'\\n'\n",
    "\n",
    "\n",
    "with open(str(parameter[\"BASE_DIR\"])+\"umi_read_list.txt\", 'wb') as f:\n",
    "                   f.write(out_data)\n",
    "    \n",
    "print \"DONE\"               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
